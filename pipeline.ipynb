{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d87ca",
   "metadata": {
    "id": "2f6d87ca"
   },
   "source": [
    "# Google Colab Turtle Recognition Pipeline - Tag a Turtle\n",
    "## 1. Introduction\n",
    "The goal of this pipeline is to identify sea turtles by their facial features using machine learning models. It processes turtle face images, creates embeddings for these faces, and matches them to known turtle identities.\n",
    "\n",
    "To learn more about the project and technical aspect of the pipeline, please read the README.md\n",
    "\n",
    "## 2. Installation instructions\n",
    "**Step 1**: Download this pipeline.ipynb file and upload it to your drive.\n",
    "\n",
    "**Step 2**: Upload the dataset to your drive if you haven't already.\n",
    "\n",
    "**Step 3**: Create a folder for images you want to indentify in your drive and upload images there.\n",
    "\n",
    "**Step 4**: Download and upload the fine-tuned model weights for the models used in this pipeline. To get a copy of the model weights contact your developers or dan.rayu@gmail.com.\n",
    "\n",
    "**Step 5**: Open the pipeline.ipynb file and fill in paths for the image dataset, the identify folder, the model weights. You will be given straightforward errors if some of these files can't be found.\n",
    "\n",
    "Congrats! You have done all the necessary preparations to use the pipeline.\n",
    "\n",
    "## 3. Usage instructions:\n",
    "**Step 1**: Insert images of unknown turtles into the predict folder found on your drive.\n",
    "\n",
    "**Step 2**:  Make sure this notebook is connected to the GPU\n",
    "\n",
    "* Check the top right corner. If next to the ✅ it says “T4” or “Connect GPU” you are good to go\n",
    "* If it says nothing, or anything other than T4 or GPU, you need to click the small arrow (▼) and select “Change runtime type”. Then in the pop-up menu you can select T4 GPU and save.\n",
    "\n",
    "**Step 3**: Now click on the play button (Run all) next to the text to start the code\n",
    "\n",
    "**Step 4**: A pop-up will show saying you need to connect to drive - accept\n",
    "\n",
    "**Step 5**: Scroll to the bottom wait for the results to show up.\n",
    "\n",
    "## Extra troubleshooting\n",
    "If the system keeps crashing, try deleting the RecognitionCache folder and all of it's contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29249256",
   "metadata": {
    "id": "29249256"
   },
   "source": [
    "## Configuration\n",
    "### Process configuration\n",
    "Parameters that impact recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "3b5f1030",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094560194,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "3b5f1030"
   },
   "outputs": [],
   "source": [
    "\n",
    "# The resized image to center crop from for the embedding model\n",
    "resize_scale = 256\n",
    "\n",
    "# Resize scale before passing to Detection model\n",
    "full_resize_n = 1920\n",
    "\n",
    "# Combine all dataset images of the same turtle into one for improved performance\n",
    "aggregate_dataset_identities = False\n",
    "\n",
    "# Combine all query images of the same turtle into one for improved performance\n",
    "aggregate_query_identities = False\n",
    "\n",
    "# The amount of most similar images to be shown (top 10, or top 5 most similar turtles)\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a278c",
   "metadata": {
    "id": "ac8a278c"
   },
   "source": [
    "### Parallelism configuration\n",
    "Options that impact system speed and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "56b03d69",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752094130226,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "56b03d69"
   },
   "outputs": [],
   "source": [
    "# Larger batch sizes speed up processing but may overflow memory.\n",
    "inference_batch_size = 4\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01705df",
   "metadata": {
    "id": "c01705df"
   },
   "source": [
    "## Mount Drive\n",
    "Uncomment for Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "f39a4816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17261,
     "status": "ok",
     "timestamp": 1752094147502,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "f39a4816",
    "outputId": "e155ba7e-1a6d-43d5-a45a-c8e5b7aa6bb2"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ec4b3",
   "metadata": {
    "id": "b22ec4b3"
   },
   "source": [
    "### Paths configuration for Google Collab\n",
    "Uncomment before using Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "9656d7f5",
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1752094147960,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "9656d7f5"
   },
   "outputs": [],
   "source": [
    "# data_path_config = \"Data\"\n",
    "# yolo_weights_config = 'best.pt'\n",
    "# embedding_weights_config = 'best_fresh-field-240.pth'\n",
    "# predict_folder_config = 'predict'\n",
    "# basedir = \"/content/drive/MyDrive/AI_for_Turtles\"\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Adjust these only for developers\n",
    "# dataset_path = os.path.join(basedir, data_path_config)\n",
    "# if not os.path.exists(dataset_path):\n",
    "#     raise FileNotFoundError(f\"Dataset path {dataset_path} does not exist.\")\n",
    "# yolo_checkpoint_path = os.path.join(basedir, yolo_weights_config)\n",
    "# if not os.path.exists(yolo_checkpoint_path):\n",
    "#     raise FileNotFoundError(f\"Yolo checkpoint path {yolo_checkpoint_path} does not exist.\")\n",
    "# embedding_checkpoint_path = os.path.join(basedir, embedding_weights_config)\n",
    "# if not os.path.exists(embedding_checkpoint_path):\n",
    "#     raise FileNotFoundError(f\"Resnet101 checkpoint path {embedding_checkpoint_path} does not exist.\")\n",
    "# query_folder_path = os.path.join(basedir, predict_folder_config)\n",
    "# if not os.path.exists(query_folder_path):\n",
    "#     raise FileNotFoundError(f\"Query folder path {query_folder_path} does not exist.\")\n",
    "\n",
    "# # Path of the embedded dataset. This embedding file has to be regenerated if the model is updated.\n",
    "# saved_embeddings_path = os.path.join(basedir, \"RecognitionCache/embeddings/embeddings.pth\")\n",
    "# # Path of the generated metadata\n",
    "# auto_metadata_path =  os.path.join(basedir, f\"RecognitionCache/datasets/auto_{dataset_path.split('/')[-1]}.csv\")\n",
    "# metadata_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8267e",
   "metadata": {
    "id": "b9d8267e"
   },
   "source": [
    "## Path configuration for PC\n",
    "Comment for Google Collab running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "f17da698",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094147962,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "f17da698"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/home/delta/Documents/Turtles/dataset_May15th/train/reid\"\n",
    "yolo_checkpoint_path = '/home/delta/Documents/Turtles/dataset_ops/Tag-A-Turtle/weights/best.pt'\n",
    "embedding_checkpoint_path = \"/home/delta/Documents/Turtles/inference_pipeline/best_fresh-field-240.pth\"\n",
    "side_detector_checkpoint_path = \"/home/delta/Documents/Turtles/inference_pipeline/checkpoint_epoch_40.pth\"\n",
    "query_folder_path = \"/home/delta/Documents/Turtles/Proxy-Anchor/query_images\"\n",
    "\n",
    "# Path of the embedded dataset. This embedding file has to be regenerated if the model is updated.\n",
    "saved_embeddings_path = \"./RecognitionCache/embeddings/embeddings.pth\"\n",
    "# Path of the generated metadata\n",
    "auto_metadata_path = f\"/home/delta/Documents/Turtles/inference_pipeline/RecognitionCache/datasets/auto_{dataset_path.split('/')[-1]}.csv\"\n",
    "# Path of images in the same order as the embeddings\n",
    "embedding_image_path_list_path = f\"/home/delta/Documents/Turtles/inference_pipeline/RecognitionCache/embeddings/embedding_image_path_list.csv\"\n",
    "metadata_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949bb9a",
   "metadata": {
    "id": "4949bb9a"
   },
   "source": [
    "### Testing configuration\n",
    "Only for developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "096becda",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094147965,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "096becda"
   },
   "outputs": [],
   "source": [
    "limit_dataset_images_testing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "ed2c44e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094147967,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "ed2c44e9"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "error_crops = 0\n",
    "def set_error_crops(n):\n",
    "    global error_crops\n",
    "    error_crops = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390669c",
   "metadata": {
    "id": "3390669c"
   },
   "source": [
    "## Install dependencies\n",
    "Uncomment for Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "7e3c0140",
   "metadata": {
    "executionInfo": {
     "elapsed": 134678,
     "status": "ok",
     "timestamp": 1752094282647,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "7e3c0140"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# %%capture\n",
    "# !pip install torch tqdm torchvision pandas matplotlib Pillow ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b416833",
   "metadata": {
    "id": "6b416833"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "58828243",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41780,
     "status": "ok",
     "timestamp": 1752094324425,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "58828243",
    "outputId": "236cfd63-a8bc-4117-8a0b-6b1bdb0cc03e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageOps\n",
    "from ultralytics import YOLO\n",
    "from transformers import Dinov2Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "# determine device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68a98",
   "metadata": {
    "id": "71c68a98"
   },
   "source": [
    "## Set the device on which to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "2d205147",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752094324433,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "2d205147",
    "outputId": "0c08025a-3bb1-4ac0-884e-bd3eb79a44fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to 'cuda' if a GPU is available, otherwise default to 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device if use_gpu else \"cpu\"\n",
    "if use_gpu and device == \"cpu\":\n",
    "    print(\"GPU not available.\")\n",
    "# Verify the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7c7ed",
   "metadata": {
    "id": "d9d7c7ed"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "9c63a8ef",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094324435,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "9c63a8ef"
   },
   "outputs": [],
   "source": [
    "def extract_labels(file_path):\n",
    "    # Extract the filename from the full file path\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    # Check for format 1: \"03_066_R_2003_1.jpg\" (any extension, underscores included)\n",
    "    match1 = re.match(r\"(\\d{2}[-_]\\d{3})_([A-Za-z])_\\d{4}_\\d+\\.[a-zA-Z]+$\", filename)\n",
    "    if match1:\n",
    "        return [match1.group(1), match1.group(2)]\n",
    "\n",
    "    # Check for format 2: \"03_066 R.JPG\" (any extension, underscores included)\n",
    "    match2 = re.match(r\"(\\d{2}[-_]\\d{3})\\s([A-Za-z])\\.[a-zA-Z]+$\", filename)\n",
    "\n",
    "    match3 = re.match(r\"(.*\\s)?(\\d{2}[-_]\\d{3})\\s([A-Za-z])\\.[a-zA-Z]+$\", filename)\n",
    "\n",
    "    if match2:\n",
    "        return [match2.group(1), match2.group(2)]\n",
    "\n",
    "    if match3:\n",
    "      return [match3.group(1), match3.group(2)]\n",
    "\n",
    "    # Return None if no match found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "bee423ab",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094324438,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "bee423ab"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./RecognitionCache/datasets\", exist_ok=True)\n",
    "os.makedirs(\"./RecognitionCache/embeddings\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84625b",
   "metadata": {
    "id": "2a84625b"
   },
   "source": [
    "## Generating a metadata for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "0e7947da",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094324441,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "0e7947da"
   },
   "outputs": [],
   "source": [
    "def generate_data_folder_metadata(root_path):\n",
    "    file_paths = []\n",
    "\n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    data = []\n",
    "    for file_path in file_paths:\n",
    "        labels = extract_labels(file_path)\n",
    "        # Add the row to the new format data\n",
    "        if labels:\n",
    "          data.append({\n",
    "              'bonaire_turtle_id': labels[0],\n",
    "              'side': labels[1],\n",
    "              'filename': file_path,\n",
    "          })\n",
    "    if limit_dataset_images_testing:\n",
    "        data = data[:16]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6ae8",
   "metadata": {
    "id": "9a8e6ae8"
   },
   "source": [
    "## Generating metadata for the query files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "fbfb09d0",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094324442,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "fbfb09d0"
   },
   "outputs": [],
   "source": [
    "def generate_query_folder_metadata(root_path):\n",
    "    data = []\n",
    "\n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            data.append({\n",
    "                'bonaire_turtle_id': f\"{subdir}\",\n",
    "                'side': \"U\",\n",
    "                'filename': os.path.join(subdir, file),\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf0522",
   "metadata": {
    "id": "5edf0522"
   },
   "source": [
    "## Make sure there is metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "faae87e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752094324445,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "faae87e2"
   },
   "outputs": [],
   "source": [
    "def ensure_metadata():\n",
    "    data_path = None\n",
    "\n",
    "    # Try loading user provided metadata\n",
    "    if metadata_path:\n",
    "        data_path = metadata_path\n",
    "\n",
    "    # If it fails, load auto metadata\n",
    "    else:\n",
    "        data_path = auto_metadata_path\n",
    "    data_exists = os.path.isfile(data_path)\n",
    "\n",
    "    # If there is no metadata, generate it\n",
    "    if not data_exists:\n",
    "        print(f\"There is no metadata file {data_path}\")\n",
    "        generate_data_folder_metadata(dataset_path)\n",
    "        data_path = auto_metadata_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629559f",
   "metadata": {
    "id": "a629559f"
   },
   "source": [
    "## Resize image tensor batch from any size to size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "0c672224",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752094324446,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "0c672224"
   },
   "outputs": [],
   "source": [
    "def resize_image_t_to_n_padded(image_batch, n=224):\n",
    "    \"\"\"\n",
    "    Takes a batch of image tensors, resizes the smaller dimension to 'n', and pads\n",
    "    the larger dimension with black to fit a square of size n x n. Returns a batch of\n",
    "    images in tensor form.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): A tensor batch of images with shape (B, C, H, W), where B is the batch size,\n",
    "                                   C is the number of channels (3 for RGB), H is the height, and W is the width.\n",
    "        n (int): The target size for the square image (default 224).\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor batch of images, each of size n x n.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((n, n)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    batch_images = []\n",
    "    for img_tensor in image_batch:\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        width, height = img.size\n",
    "        if width == height:\n",
    "            # If the image is already square, simply resize\n",
    "            transformed_image = transform(img)\n",
    "        elif width < height:\n",
    "            # Resize height to n, calculate padding for width\n",
    "            new_height = n\n",
    "            new_width = int(width * (new_height / height))\n",
    "            padding_left = (n - new_width) // 2\n",
    "            padding_right = n - new_width - padding_left\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((padding_left, 0, padding_right, 0), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img)\n",
    "        else:  # height < width\n",
    "            # Resize width to n, calculate padding for height\n",
    "            new_width = n\n",
    "            new_height = int(height * (new_width / width))\n",
    "            padding_top = (n - new_height) // 2\n",
    "            padding_bottom = n - new_height - padding_top\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((0, padding_top, 0, padding_bottom), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img)\n",
    "\n",
    "        batch_images.append(transformed_image)\n",
    "\n",
    "    # Stack images into a single tensor batch\n",
    "    return torch.stack(batch_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "07a1253f",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094324450,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "07a1253f"
   },
   "outputs": [],
   "source": [
    "def resize_image_to_n_padded(image_pil, n=224):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((n, n)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = image_pil\n",
    "\n",
    "    width, height = img.size\n",
    "    if width == height:\n",
    "        # If the image is already square, simply resize\n",
    "        transformed_image = transform(img)\n",
    "    elif width < height:\n",
    "        # Resize height to n, calculate padding for width\n",
    "        new_height = n\n",
    "        new_width = int(width * (new_height / height))\n",
    "        padding_left = (n - new_width) // 2\n",
    "        padding_right = n - new_width - padding_left\n",
    "        transformed_image = transforms.Compose([\n",
    "            transforms.Resize((new_height, new_width)),\n",
    "            transforms.Pad((padding_left, 0, padding_right, 0), fill=0, padding_mode='constant'),\n",
    "            transforms.ToTensor()\n",
    "        ])(img)\n",
    "    else:  # height < width\n",
    "        # Resize width to n, calculate padding for height\n",
    "        new_width = n\n",
    "        new_height = int(height * (new_width / width))\n",
    "        padding_top = (n - new_height) // 2\n",
    "        padding_bottom = n - new_height - padding_top\n",
    "        transformed_image = transforms.Compose([\n",
    "            transforms.Resize((new_height, new_width)),\n",
    "            transforms.Pad((0, padding_top, 0, padding_bottom), fill=0, padding_mode='constant'),\n",
    "            transforms.ToTensor()\n",
    "        ])(img)\n",
    "\n",
    "\n",
    "    # Stack images into a single tensor batch\n",
    "    return transformed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26222d2b",
   "metadata": {
    "id": "26222d2b"
   },
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "c4ac280e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752094324453,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "c4ac280e"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TurtlesPathDataset(Dataset):\n",
    "    def __init__(self, mode: str, root: str = None, transform=None, ignoreThreshold=0, data=None):\n",
    "        self.mode = mode.lower()\n",
    "        self.transform = transform\n",
    "        if data is None:\n",
    "            data_path = metadata_path if metadata_path else auto_metadata_path\n",
    "            self.root = root\n",
    "            meta_path = os.path.join(self.root, data_path)\n",
    "            if not os.path.isfile(meta_path):\n",
    "                raise FileNotFoundError(f\"No metadata file found (expected {data_path}).\")\n",
    "            data_df = pd.read_csv(meta_path)\n",
    "        else:\n",
    "            data_df = data\n",
    "\n",
    "        if limit_dataset_images_testing:\n",
    "            cutoff = min(16, len(data_df))\n",
    "            data_df = data_df[:cutoff]\n",
    "\n",
    "        self.im_paths, self._y_strs, self.positions = [], [], []\n",
    "        for _, (turtle_id, side, filename) in data_df.iterrows():\n",
    "                img_path = filename\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "\n",
    "                identity = f\"{turtle_id}\"\n",
    "                self.im_paths.append(img_path)\n",
    "                self._y_strs.append(identity)\n",
    "                self.positions.append(side)\n",
    "\n",
    "        if not self.im_paths:\n",
    "            raise RuntimeError(\"Dataset is empty.\")\n",
    "\n",
    "        all_classes = set(self._y_strs)\n",
    "\n",
    "        self.classes = all_classes\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.ys = [self.class_to_idx[s] for s in self._y_strs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.im_paths[index], self.ys[index]\n",
    "\n",
    "    def get_path(self, index):\n",
    "        return self.im_paths[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e39fe",
   "metadata": {
    "id": "839e39fe"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f8269d1",
   "metadata": {
    "id": "1f8269d1"
   },
   "source": [
    "## Get Labels from Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "9832a025",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094324461,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "9832a025"
   },
   "outputs": [],
   "source": [
    "def get_dataloader_values(dataloader):\n",
    "    \"\"\"\n",
    "    This function takes a PyTorch DataLoader and returns a tensor containing all the labels.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all the labels.\n",
    "    \"\"\"\n",
    "    final_labels = [], []\n",
    "    all_labels = []\n",
    "\n",
    "    for paths, labels in dataloader:\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    final_labels = torch.tensor(all_labels)\n",
    "    return final_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee3225",
   "metadata": {
    "id": "62ee3225"
   },
   "source": [
    "## Face Side Detection Implementation\n",
    "A resnet18 trained to detect turtle face sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "dab3664d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094324464,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "dab3664d"
   },
   "outputs": [],
   "source": [
    "class FaceSideModel(nn.Module):\n",
    "    def __init__(self, pretrained=True, is_norm=True, bn_freeze=True):\n",
    "\n",
    "        super(FaceSideModel, self).__init__()\n",
    "\n",
    "        self.model = resnet18(pretrained=pretrained)\n",
    "\n",
    "        self.is_norm = is_norm\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Replace the last fully connected layer with a new one for 2 class output\n",
    "        self.model.fc = nn.Linear(self.num_ftrs, 2)  # 2 classes: L and R\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = avg_x + max_x  # Combine average and max pooling features\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.fc(x)  # Final output layer\n",
    "\n",
    "        if self.is_norm:\n",
    "            x = self.l2_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def l2_norm(self, input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "        return output\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Only initialize the newly added fc layer\n",
    "        if hasattr(self.model, 'fc') and isinstance(self.model.fc, nn.Linear):\n",
    "            init.kaiming_normal_(self.model.fc.weight, mode='fan_out')\n",
    "            init.constant_(self.model.fc.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bee72",
   "metadata": {
    "id": "fe0bee72"
   },
   "source": [
    "## Resnet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "aed3175d",
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1752094324496,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "aed3175d"
   },
   "outputs": [],
   "source": [
    "class Resnet101(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, is_norm=True, bn_freeze = True):\n",
    "        super(Resnet101, self).__init__()\n",
    "\n",
    "        self.model = resnet101(pretrained)\n",
    "        self.is_norm = is_norm\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.model.embedding = nn.Linear(self.num_ftrs, self.embedding_size)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "        If no object is detected, return the original image for the respective batch item.\n",
    "\n",
    "        Args:\n",
    "            image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "            The image batch should be normalized to\n",
    "            resnet_mean = [0.485, 0.456, 0.406]\n",
    "            resnet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "        \"\"\"\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.embedding(x)\n",
    "\n",
    "        if self.is_norm:\n",
    "            x = self.l2_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding.weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b387f86",
   "metadata": {
    "id": "5b387f86"
   },
   "source": [
    "## Dino Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "256958e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094324502,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "256958e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dinov2Small(nn.Module):\n",
    "    def __init__(self, embedding_size, pretrained=True, is_norm=True, bn_freeze=True):\n",
    "        super(Dinov2Small, self).__init__()\n",
    "\n",
    "        self.is_norm = is_norm\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Load pretrained DINOv2 backbone\n",
    "        self.model = Dinov2Model.from_pretrained(\"facebook/dinov2-small\")\n",
    "\n",
    "        # Feature size from CLS token\n",
    "        self.num_ftrs = self.model.config.hidden_size\n",
    "\n",
    "        # Projection head\n",
    "        self.model.embedding = nn.Linear(self.num_ftrs, self.embedding_size)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def l2_norm(self, input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "        output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        return output.view(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get CLS token\n",
    "        x = self.model(pixel_values=x).last_hidden_state[:, 0]  # shape: (B, hidden_size)\n",
    "\n",
    "        x = self.model.embedding(x)\n",
    "\n",
    "        if self.is_norm:\n",
    "            x = self.l2_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding.weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3fd8c",
   "metadata": {
    "id": "98c3fd8c"
   },
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "id": "e7ca20e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "referenced_widgets": [
      "43b784e7ac954746a4cc67cdb8c39623",
      "ee035009f16e4198bc963791ddd91fe0",
      "dad467b8bc1b48b6825d3329d6cc42de",
      "caa3f1ffc4734050af2332a8b1c4715c",
      "9e73a7f82ee445b0a821d56d9039c3e4",
      "3f800a29a2cd454da5c164072c5ab374",
      "0d7d3dc225ad462998e23a4137489679",
      "4d1a2215d74645a0add99885e9a9800c",
      "0eb27952325f4fc19dd096db42e76f27",
      "109937c35b2741c3bfed1eb5b24b31c3",
      "0f12cf0b58e540b184be33d81da11aa6",
      "69b3704557394570949018e38df51caf",
      "b0c3399c7041455a81dff4bff39c9db6",
      "3617b503cefa4dbcb76ad20b5a5fd6b6",
      "39591d9710bd4cef9cb8f345e413111b",
      "a2ecb25db43c49cabc91ceb993fd6fe6",
      "4f61bcc403dc4390a015fdb2f71ffc13",
      "50f372b29cff4719b4f757e1dbeaea86",
      "d6fff8b41575401a8bb7f30e1efbaa35",
      "c1127e836a2b47509e2c6c54700cff21",
      "518285e0ef944888893079580af6e5d0",
      "c7c34732bcc54e088ac6365654018625"
     ]
    },
    "executionInfo": {
     "elapsed": 9446,
     "status": "ok",
     "timestamp": 1752094333946,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "e7ca20e3",
    "outputId": "2b07b3ff-5994-491d-c561-e0cc70a4d9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Small(\n",
       "  (model): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2SdpaAttention(\n",
       "            (attention): Dinov2SdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (embedding): Linear(in_features=384, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1027,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_model = Resnet101(embedding_size=512, pretrained=True, is_norm=True, bn_freeze = True)\n",
    "embedding_model = Dinov2Small(embedding_size=512, pretrained=True, is_norm=True, bn_freeze = True)\n",
    "# Path to checkpoint\n",
    "checkpoint = torch.load(embedding_checkpoint_path)\n",
    "embedding_model.load_state_dict(checkpoint)\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d87e7",
   "metadata": {
    "id": "e00d87e7"
   },
   "source": [
    "## YOLO Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "c8d25a0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4911,
     "status": "ok",
     "timestamp": 1752094338858,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "c8d25a0b",
    "outputId": "ad56beeb-371a-4952-883c-ea6ed647b419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delta/Documents/Turtles/dataset_ops/Tag-A-Turtle/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "print(yolo_checkpoint_path)\n",
    "yolo_model = YOLO(yolo_checkpoint_path).cuda()  # You can specify a different model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93814c",
   "metadata": {
    "id": "0c93814c"
   },
   "source": [
    "## Single Yolo Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "id": "0d962cde",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752094338860,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "0d962cde"
   },
   "outputs": [],
   "source": [
    "def single_crop_yolov11(image_name: str, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    global error_crops\n",
    "    \"\"\"\n",
    "    Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "    If no object is detected, return the original image for the respective batch item.\n",
    "    Additionally, display the image with bounding boxes if there are more than 2 boxes.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        model (YOLO): Preloaded YOLOv11 model.\n",
    "        conf_threshold (float): Confidence threshold for detections.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image batch\n",
    "    # Perform inference\n",
    "    results = model.predict(source=image_name, conf=conf_threshold, device=device, verbose=False)\n",
    "    img = Image.open(image_name).convert(\"RGB\")\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    # Initialize a list to store cropped images\n",
    "    for result in results:\n",
    "        if result.boxes is not None and len(result.boxes.xyxy) > 0:\n",
    "            # Get the total number of boxes\n",
    "            num_boxes = len(result.boxes.xyxy)\n",
    "            if num_boxes != 1:\n",
    "                error_crops = error_crops + 1\n",
    "            # Get the first detected box (if there is at least one detection)\n",
    "            first_box = result.boxes.xyxy[0].int().cpu().tolist()  # Convert to a list of integers (x_min, y_min, x_max, y_max)\n",
    "            x_min, y_min, x_max, y_max = first_box\n",
    "\n",
    "            img = img.crop((x_min, y_min, x_max, y_max))\n",
    "        else: error_crops += 1\n",
    "\n",
    "    img = resize_image_to_n_padded(img, resize_scale)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "d1d7d1ea",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1752094338867,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "d1d7d1ea"
   },
   "outputs": [],
   "source": [
    "def batch_crop_name(image_names: torch.Tensor, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    cropped_images = []\n",
    "    for image_name in image_names:\n",
    "        cropped_image = single_crop_yolov11(image_name, model, conf_threshold, device)\n",
    "        cropped_images.append(cropped_image)\n",
    "\n",
    "    return torch.stack(cropped_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340af22",
   "metadata": {
    "id": "9340af22"
   },
   "source": [
    "## Standard Transform before Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "a031037c",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1752094338868,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "a031037c"
   },
   "outputs": [],
   "source": [
    "def make_transform(original = 256, output=224):\n",
    "    resnet_sz_resize = original\n",
    "    resnet_sz_crop = output\n",
    "    resnet_mean = [0.485, 0.456, 0.406]\n",
    "    resnet_std = [0.229, 0.224, 0.225]\n",
    "    resnet_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Resize(resnet_sz_resize),\n",
    "        transforms.CenterCrop(resnet_sz_crop),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=resnet_mean, std=resnet_std)\n",
    "    ])\n",
    "    return resnet_transform\n",
    "resnet_transform = make_transform(original=resize_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c211bc",
   "metadata": {
    "id": "d8c211bc"
   },
   "source": [
    "## Resize, crop and normalize tensors for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "66b44369",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094338868,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "66b44369"
   },
   "outputs": [],
   "source": [
    "def pre_embed_transform(image_tensor_batch):\n",
    "    processed_images = []\n",
    "    for image_tensor in image_tensor_batch:\n",
    "        processed_images.append(resnet_transform(image_tensor))\n",
    "    return torch.stack(processed_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe978b1",
   "metadata": {
    "id": "abe978b1"
   },
   "source": [
    "## Inference Pipeline\n",
    "Handles tensorized images in batches, applying all the inferences and transforms required to produce the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "id": "540ce419",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094338870,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "540ce419"
   },
   "outputs": [],
   "source": [
    "def infer_pipeline(image_tensor):\n",
    "    global yolo_model, embedding_model, error_crops\n",
    "    # Detect face in the image\n",
    "    # faces_tensor = batch_crop_yolov11(image_tensor, yolo_model)\n",
    "    faces_tensor = batch_crop_name(image_tensor, yolo_model, conf_threshold=0.5)\n",
    "    # print(\"Number of cropping errors\", error_crops)\n",
    "\n",
    "    # crop and normalize before embedding model\n",
    "    normalized_face_tensor = pre_embed_transform(faces_tensor).to(device)\n",
    "    del faces_tensor\n",
    "\n",
    "    # Generate embedding for the detected face\n",
    "    embeddings = embedding_model(normalized_face_tensor)\n",
    "    del normalized_face_tensor\n",
    "\n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "id": "36984039",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094338870,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "36984039"
   },
   "outputs": [],
   "source": [
    "def getErrorRate():\n",
    "    global error_crops\n",
    "    return error_crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf8885",
   "metadata": {
    "id": "7bdf8885"
   },
   "source": [
    "## Function to run the inference pipeline over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "66bd90a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094338876,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "66bd90a7"
   },
   "outputs": [],
   "source": [
    "def process_images_to_embeddings(data_loader):\n",
    "    global inference_batch_size\n",
    "    processed_batches = []\n",
    "    pbar = tqdm(data_loader, desc=\"Processing data\")\n",
    "    index = 1\n",
    "    for values in pbar:\n",
    "        error_rate_percentage = (getErrorRate() / (index * inference_batch_size)) * 100\n",
    "        formatted_error_rate = f\"{error_rate_percentage:.3f}%\"\n",
    "        pbar.set_postfix({\"Err Rate\": formatted_error_rate})\n",
    "        processed_batch = infer_pipeline(values[0])\n",
    "        processed_batches.append(processed_batch.cpu())\n",
    "        index += 1\n",
    "\n",
    "    return torch.cat(processed_batches, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945861d0",
   "metadata": {
    "id": "945861d0"
   },
   "source": [
    "## Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "366d5f0e",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094338877,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "366d5f0e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def turtle_similarities(query_embedding, gallery_embeddings):\n",
    "    cosine_sim = F.cosine_similarity(query_embedding, gallery_embeddings)\n",
    "    return cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "id": "cd42b783",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752094338878,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "cd42b783"
   },
   "outputs": [],
   "source": [
    "def similarity_score(similarity_value):\n",
    "    if similarity_value < 0:\n",
    "        return 0\n",
    "    if similarity_value > 1:\n",
    "        return 100\n",
    "    return similarity_value * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "id": "c7140808",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752094338878,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "c7140808"
   },
   "outputs": [],
   "source": [
    "def pil_to_numpy(pil_image):\n",
    "    return np.array(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76dd50e",
   "metadata": {
    "id": "f76dd50e"
   },
   "source": [
    "## Open plotlib images right side up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "id": "a9076757",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094338880,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "a9076757"
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_correctly(image_path):\n",
    "    global yolo_model\n",
    "    cropped_image_tensor = single_crop_yolov11(image_path, yolo_model)\n",
    "    pil_query_image = transforms.ToPILImage()(cropped_image_tensor)\n",
    "    # Convert back to NumPy array for matplotlib\n",
    "    return np.array(pil_query_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39bed8",
   "metadata": {
    "id": "9d39bed8"
   },
   "source": [
    "## Show recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "id": "af415e1d",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752094338882,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "af415e1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_recognition_results(query_image, similar_images, titles, similarity):\n",
    "    # Increase the figure size to make images 3x larger than the previous size\n",
    "    plt.figure(figsize=(90, 45))  # 3x larger than the previous figsize\n",
    "    plt.subplot(1, len(similar_images)+1, 1)\n",
    "    print(query_image[0])\n",
    "\n",
    "    if type(query_image[0]) == str:\n",
    "        query_image = display_correctly(query_image[0])\n",
    "    plt.imshow(query_image)\n",
    "    plt.title(\"Query\")\n",
    "\n",
    "    for idx, sim_img in enumerate(similar_images):\n",
    "        plt.subplot(1, len(similar_images)+1, idx+2)\n",
    "        if type(sim_img[0]) == str:\n",
    "            sim_img = display_correctly(sim_img)\n",
    "        plt.imshow(sim_img)\n",
    "        plt.title(titles[idx])\n",
    "\n",
    "        # Display similarity below each similar image\n",
    "        plt.text(0.5, -0.1, f\"Similarity: {similarity_score(similarity[idx]):.1f}%\", ha='center', va='top', transform=plt.gca().transAxes, fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1ab1",
   "metadata": {
    "id": "c95e1ab1"
   },
   "source": [
    "## Embeddings aggregator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "1cd8e8d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752094338884,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "1cd8e8d8"
   },
   "outputs": [],
   "source": [
    "class EmbeddingAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Average embeddings in the batch per turtle identity to produce more accurate embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : str\n",
    "        'mean' (default) or 'weighted'.\n",
    "        When 'weighted', `weights` must be supplied in forward().\n",
    "    return_index_map : bool\n",
    "        If True, also returns a tensor that maps each original sample\n",
    "        to its aggregated row – handy for gathering losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = \"mean\", return_index_map: bool = False):\n",
    "        super().__init__()\n",
    "        assert reduction in {\"mean\", \"weighted\"}\n",
    "        self.reduction = reduction\n",
    "        self.return_index_map = return_index_map\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        emb: torch.Tensor,             # shape (B, D)\n",
    "        labels: torch.Tensor,          # shape (B,)\n",
    "        weights: torch.Tensor | None = None\n",
    "    ):\n",
    "        device = emb.device\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # ---- gather bookkeeping -------------------------------------------------\n",
    "        # unique ids and position of each sample’s aggregated row\n",
    "        uniq_ids, inv = torch.unique(labels, return_inverse=True)\n",
    "        num_ids = uniq_ids.size(0)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            # quick path – use scatter_add then divide by counts\n",
    "            summed = torch.zeros(num_ids, emb.size(1), device=device).scatter_add_(0,\n",
    "                      inv.unsqueeze(-1).expand_as(emb), emb)\n",
    "            counts = torch.bincount(inv, minlength=num_ids).unsqueeze(1)\n",
    "            agg = summed / counts.clamp_min(1)\n",
    "\n",
    "        else:  # weighted mean\n",
    "            assert weights is not None, \"`weights` required for weighted reduction\"\n",
    "            weights = weights.to(device).unsqueeze(1)  # (B, 1)\n",
    "            w_sum = torch.zeros(num_ids, 1, device=device).scatter_add_(0, inv.unsqueeze(-1), weights)\n",
    "            w_emb = torch.zeros_like(summed).scatter_add_(0, inv.unsqueeze(-1).expand_as(emb), emb * weights)\n",
    "            agg = w_emb / w_sum.clamp_min(1e-8)\n",
    "\n",
    "        if self.return_index_map:\n",
    "            return agg, uniq_ids, inv\n",
    "        return agg, uniq_ids, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6665ca3",
   "metadata": {
    "id": "b6665ca3"
   },
   "source": [
    "## Sort embeddings by similarity with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "id": "cbcb8345",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752094338886,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "cbcb8345"
   },
   "outputs": [],
   "source": [
    "def sort_em_by_sim(similarities, query_labels):\n",
    "    similarities_and_labels = [(similarity.item(), query_labels[i]) for i, similarity in enumerate(similarities)]\n",
    "\n",
    "    # Sort the list by similarity in descending order\n",
    "    similarities_and_labels_sorted = sorted(similarities_and_labels, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Optionally, separate the sorted similarities and labels back into two lists/tensors\n",
    "    sorted_similarities = [pair[0] for pair in similarities_and_labels_sorted]\n",
    "    sorted_labels = [pair[1] for pair in similarities_and_labels_sorted]\n",
    "\n",
    "    return sorted_similarities, sorted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6a06c",
   "metadata": {
    "id": "c6b6a06c"
   },
   "source": [
    "## Retrieve the images in a dataset that match the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "id": "5760ba6b",
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1752094338888,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "5760ba6b"
   },
   "outputs": [],
   "source": [
    "def find_matching_indices(string_list, query):\n",
    "    # Create a list of indices where the string matches the query\n",
    "    matching_indices = [i for i, s in enumerate(string_list) if s == query]\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b2979",
   "metadata": {
    "id": "139b2979"
   },
   "source": [
    "## Retrieve cropped images from dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be01ec",
   "metadata": {
    "id": "b7be01ec"
   },
   "source": [
    "## Basic tensor transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "id": "a9cc593c",
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1752094338973,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "a9cc593c"
   },
   "outputs": [],
   "source": [
    "to_tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b7cb65",
   "metadata": {},
   "source": [
    "## Path list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "8be3d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_image_path_list():\n",
    "    global embedding_image_path_list_path\n",
    "    path = embedding_image_path_list_path\n",
    "    if not os.path.isfile(path):\n",
    "        return pd.DataFrame([])\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca1e9",
   "metadata": {
    "id": "821ca1e9"
   },
   "source": [
    "## Detect new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "e4e11a64",
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1752094338973,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "e4e11a64"
   },
   "outputs": [],
   "source": [
    "def get_new_images():\n",
    "    old_metadata = None\n",
    "    # determine new images\n",
    "    if os.path.exists(auto_metadata_path):\n",
    "        old_metadata = pd.read_csv(auto_metadata_path)\n",
    "        print(old_metadata.head())\n",
    "\n",
    "        old_dataset = TurtlesPathDataset(root=None, mode=\"all\", transform=to_tensor_transform, data=old_metadata)\n",
    "\n",
    "        new_metadata = generate_data_folder_metadata(dataset_path)\n",
    "        new_dataset = TurtlesPathDataset(root=None, mode=\"all\", transform=to_tensor_transform, data=new_metadata)\n",
    "\n",
    "        new_images_metadata = pd.DataFrame(columns=['bonaire_turtle_id','side', 'filename'])\n",
    "        new_images_metadata_index = 0\n",
    "        for index in range(0, len(new_dataset)):\n",
    "            row = new_dataset[index]\n",
    "            if row[0] not in old_dataset.im_paths:\n",
    "                # print new image\n",
    "                print(\"pppp\",[row[1], \"U\", row[0]])\n",
    "                new_images_metadata.loc[new_images_metadata_index] = [row[1], \"U\", row[0]]\n",
    "                new_images_metadata_index += 1\n",
    "    else:\n",
    "        new_images_metadata = generate_data_folder_metadata(dataset_path)\n",
    "    return new_images_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2522ca",
   "metadata": {
    "id": "4a2522ca"
   },
   "source": [
    "## Generate embeddings for new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "id": "ae22e0c4",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752094338974,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "ae22e0c4"
   },
   "outputs": [],
   "source": [
    "def embed_images_from_df(df):\n",
    "    new_images_dataset = TurtlesPathDataset(data=df, mode=\"all\", transform=to_tensor_transform)\n",
    "    new_images_data_loader = DataLoader(new_images_dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "    image_labels = get_dataloader_values(new_images_data_loader)\n",
    "    new_images_embeddings = process_images_to_embeddings(new_images_data_loader)\n",
    "    \n",
    "    img_paths = new_images_dataset.im_paths\n",
    "    return new_images_embeddings, image_labels, pd.DataFrame(img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109405ba",
   "metadata": {
    "id": "109405ba"
   },
   "source": [
    "## Main Pipeline Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "id": "7830ed18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554,
     "output_embedded_package_id": "1D2tTaeRoLM8hZKRAPsdFCJlroEqyOk7O"
    },
    "executionInfo": {
     "elapsed": 16022,
     "status": "ok",
     "timestamp": 1752094860220,
     "user": {
      "displayName": "Delta",
      "userId": "10263519751113061914"
     },
     "user_tz": -120
    },
    "id": "7830ed18",
    "outputId": "b8e7f6b8-3898-4cb3-e686-88b7135343e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bonaire_turtle_id side                                           filename\n",
      "0            13-054    R  /home/delta/Documents/Turtles/dataset_May15th/...\n",
      "1            13-054    L  /home/delta/Documents/Turtles/dataset_May15th/...\n",
      "2            24-191    L  /home/delta/Documents/Turtles/dataset_May15th/...\n",
      "3            24-191    R  /home/delta/Documents/Turtles/dataset_May15th/...\n",
      "4            24-191    L  /home/delta/Documents/Turtles/dataset_May15th/...\n",
      "Dataset embeddings loaded.\n",
      "Found 3 query images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, Err Rate=0.000%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3225)\n",
      "                                                      0  \\\n",
      "0     /home/delta/Documents/Turtles/dataset_May15th/...   \n",
      "1     /home/delta/Documents/Turtles/dataset_May15th/...   \n",
      "2     /home/delta/Documents/Turtles/dataset_May15th/...   \n",
      "3     /home/delta/Documents/Turtles/dataset_May15th/...   \n",
      "4     /home/delta/Documents/Turtles/dataset_May15th/...   \n",
      "...                                                 ...   \n",
      "8408                                                NaN   \n",
      "8409                                                NaN   \n",
      "8410                                                NaN   \n",
      "8411                                                NaN   \n",
      "8412                                                NaN   \n",
      "\n",
      "                                                    0.1  \n",
      "0                                                   NaN  \n",
      "1                                                   NaN  \n",
      "2                                                   NaN  \n",
      "3                                                   NaN  \n",
      "4                                                   NaN  \n",
      "...                                                 ...  \n",
      "8408  /home/delta/Documents/Turtles/dataset_May15th/...  \n",
      "8409  /home/delta/Documents/Turtles/dataset_May15th/...  \n",
      "8410  /home/delta/Documents/Turtles/dataset_May15th/...  \n",
      "8411  /home/delta/Documents/Turtles/dataset_May15th/...  \n",
      "8412  /home/delta/Documents/Turtles/dataset_May15th/...  \n",
      "\n",
      "[8413 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1048], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Sort the similarities and get the top N results\u001b[39;00m\n\u001b[1;32m     82\u001b[0m top_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(top_n, \u001b[38;5;28mlen\u001b[39m(similarities))\n\u001b[0;32m---> 83\u001b[0m sorted_similarities, sorted_locations \u001b[38;5;241m=\u001b[39m \u001b[43msort_em_by_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_locations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m sorted_locations \u001b[38;5;241m=\u001b[39m sorted_locations[:top_n]\n\u001b[1;32m     85\u001b[0m sorted_similarities \u001b[38;5;241m=\u001b[39m sorted_similarities[:top_n]\n",
      "Cell \u001b[0;32mIn[1042], line 2\u001b[0m, in \u001b[0;36msort_em_by_sim\u001b[0;34m(similarities, query_labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msort_em_by_sim\u001b[39m(similarities, query_labels):\n\u001b[0;32m----> 2\u001b[0m     similarities_and_labels \u001b[38;5;241m=\u001b[39m [(similarity\u001b[38;5;241m.\u001b[39mitem(), \u001b[43mquery_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i, similarity \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(similarities)]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Sort the list by similarity in descending order\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     similarities_and_labels_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(similarities_and_labels, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "with torch.no_grad():\n",
    "    error_crops = 0\n",
    "    # Load dataset embeddings\n",
    "    dataset_embeddings = None\n",
    "    dataset_image_embeddings, dataset_embedded_image_labels = torch.tensor([]), torch.tensor([])\n",
    "    dataset_paths_list = pd.DataFrame([])\n",
    "    new_images = get_new_images()\n",
    "    embeddings_exist = os.path.exists(saved_embeddings_path)\n",
    "    current_metadata = generate_data_folder_metadata(dataset_path)\n",
    "\n",
    "    dataset = TurtlesPathDataset(root=None, mode=\"all\", transform=to_tensor_transform, data=current_metadata)\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "    ne = torch.tensor([])\n",
    "    if embeddings_exist:\n",
    "        dataset_image_embeddings, dataset_embedded_image_labels = torch.load(saved_embeddings_path)\n",
    "        dataset_paths_list = get_embedding_image_path_list()\n",
    "        # torch.save((dataset_image_embeddings[:8396], dataset_embedded_image_labels[:8396]), saved_embeddings_path)\n",
    "        # dataset_paths_list[:8396].to_csv(embedding_image_path_list_path, index=False)\n",
    "        # sys.exit(0)\n",
    "\n",
    "    # add new images\n",
    "    if new_images.shape[0] > 0:\n",
    "        new_images_embeddings, new_images_labels, new_images_paths = embed_images_from_df(new_images)\n",
    "\n",
    "        temp_df_list = [dataset_paths_list, new_images_paths]\n",
    "        dataset_paths_list = pd.concat(temp_df_list)\n",
    "        dataset_image_embeddings = torch.cat([dataset_image_embeddings, new_images_embeddings], dim=0)\n",
    "        dataset_embedded_image_labels = torch.cat([dataset_embedded_image_labels, new_images_labels], dim=0)\n",
    "\n",
    "        torch.save((dataset_image_embeddings, dataset_embedded_image_labels), saved_embeddings_path)\n",
    "        dataset_paths_list.to_csv(embedding_image_path_list_path, index=False)\n",
    "        current_metadata.to_csv(auto_metadata_path, index=False)\n",
    "\n",
    "\n",
    "    print(\"Dataset embeddings loaded.\")\n",
    "    # Load query embeddings\n",
    "    query_metadata = generate_query_folder_metadata(query_folder_path)\n",
    "    print(f\"Found {len(query_metadata)} query images\")\n",
    "    query_dataset = TurtlesPathDataset(data=query_metadata, mode=\"all\", transform=to_tensor_transform)\n",
    "    query_loader = DataLoader(query_dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "    query_embeddings = process_images_to_embeddings(query_loader)\n",
    "    image_query_labels = get_dataloader_values(query_loader)\n",
    "\n",
    "    # Aggregate embeddings by identity if enabled\n",
    "    aggregator = EmbeddingAggregator().to(device)\n",
    "    # Aggregating dataset identities underperforms, so it will be disabled for now.\n",
    "    if aggregate_dataset_identities:\n",
    "        dataset_image_embeddings, dataset_embedded_image_labels, _ = aggregator(dataset_image_embeddings, labels=dataset_embedded_image_labels)\n",
    "    if aggregate_query_identities:\n",
    "        query_embeddings, query_labels, _ = aggregator(query_embeddings, labels=image_query_labels)\n",
    "\n",
    "    # tq1 = dataset_image_embeddings[-3].unsqueeze(0)\n",
    "    # tq2 = query_embeddings[-1]\n",
    "    # similarities = turtle_similarities(tq2, tq1)\n",
    "    # print(similarities)\n",
    "    # sys.exit(0)\n",
    "    # Get similarities between query and dataset embeddings\n",
    "    for query_index in range(query_embeddings.shape[0]):\n",
    "        query_em = query_embeddings[query_index]\n",
    "\n",
    "        # Get the similarities between query and dataset embeddings\n",
    "        similarities = turtle_similarities(query_em, dataset_image_embeddings)\n",
    "        print(similarities[-3])\n",
    "        print(dataset_paths_list.head(-3))\n",
    "        # Extract image paths from dataset\n",
    "        image_locations = []  # image paths\n",
    "        image_labels = dataset_embedded_image_labels.tolist()\n",
    "        # Tie aggregated dataset embedding labels to the path of the first image of that identity\n",
    "        if aggregate_dataset_identities:\n",
    "            for i in range((len(image_labels))):\n",
    "                label = image_labels[i]\n",
    "                for j in range(len(dataset)):\n",
    "                    if dataset[j][1] == label:\n",
    "                        image_locations.append(dataset[j][0])\n",
    "                        break\n",
    "        else:\n",
    "            image_locations = [dataset[i][0] for i in range(len(dataset))]\n",
    "\n",
    "        # Sort the similarities and get the top N results\n",
    "        top_n = min(top_n, len(similarities))\n",
    "        sorted_similarities, sorted_locations = sort_em_by_sim(similarities, image_locations)\n",
    "        sorted_locations = sorted_locations[:top_n]\n",
    "        sorted_similarities = sorted_similarities[:top_n]\n",
    "        print(sorted_similarities, sorted_locations)\n",
    "        print(\"\")\n",
    "        # Prepare images and filenames to be plotted\n",
    "        images = []\n",
    "        images_labels = []\n",
    "        for label in sorted_locations:\n",
    "            images.append(label)  # image path\n",
    "            images_labels.append(os.path.basename(label))  # Get the filename from the image path\n",
    "\n",
    "        # Find the query image path and plot results\n",
    "        query_image_index = query_index\n",
    "\n",
    "        if aggregate_query_identities:\n",
    "            query_label = query_labels[query_index]\n",
    "            query_image_index = find_matching_indices(image_query_labels, query_label)[0]\n",
    "            print(f\"Query turtle number: {query_index+1} out of {query_embeddings.shape[0]}\")\n",
    "        else:\n",
    "            print(f\"Query image number: {query_index+1} out of {query_embeddings.shape[0]}\")\n",
    "        query_image = query_dataset[query_image_index]\n",
    "        plot_recognition_results(query_image, images, images_labels, sorted_similarities)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d7d3dc225ad462998e23a4137489679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0eb27952325f4fc19dd096db42e76f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f12cf0b58e540b184be33d81da11aa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "109937c35b2741c3bfed1eb5b24b31c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3617b503cefa4dbcb76ad20b5a5fd6b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6fff8b41575401a8bb7f30e1efbaa35",
      "max": 88249960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c1127e836a2b47509e2c6c54700cff21",
      "value": 88249960
     }
    },
    "39591d9710bd4cef9cb8f345e413111b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_518285e0ef944888893079580af6e5d0",
      "placeholder": "​",
      "style": "IPY_MODEL_c7c34732bcc54e088ac6365654018625",
      "value": " 88.2M/88.2M [00:01&lt;00:00, 62.3MB/s]"
     }
    },
    "3f800a29a2cd454da5c164072c5ab374": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43b784e7ac954746a4cc67cdb8c39623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee035009f16e4198bc963791ddd91fe0",
       "IPY_MODEL_dad467b8bc1b48b6825d3329d6cc42de",
       "IPY_MODEL_caa3f1ffc4734050af2332a8b1c4715c"
      ],
      "layout": "IPY_MODEL_9e73a7f82ee445b0a821d56d9039c3e4"
     }
    },
    "4d1a2215d74645a0add99885e9a9800c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f61bcc403dc4390a015fdb2f71ffc13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50f372b29cff4719b4f757e1dbeaea86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "518285e0ef944888893079580af6e5d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69b3704557394570949018e38df51caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0c3399c7041455a81dff4bff39c9db6",
       "IPY_MODEL_3617b503cefa4dbcb76ad20b5a5fd6b6",
       "IPY_MODEL_39591d9710bd4cef9cb8f345e413111b"
      ],
      "layout": "IPY_MODEL_a2ecb25db43c49cabc91ceb993fd6fe6"
     }
    },
    "9e73a7f82ee445b0a821d56d9039c3e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2ecb25db43c49cabc91ceb993fd6fe6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0c3399c7041455a81dff4bff39c9db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f61bcc403dc4390a015fdb2f71ffc13",
      "placeholder": "​",
      "style": "IPY_MODEL_50f372b29cff4719b4f757e1dbeaea86",
      "value": "model.safetensors: 100%"
     }
    },
    "c1127e836a2b47509e2c6c54700cff21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7c34732bcc54e088ac6365654018625": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caa3f1ffc4734050af2332a8b1c4715c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_109937c35b2741c3bfed1eb5b24b31c3",
      "placeholder": "​",
      "style": "IPY_MODEL_0f12cf0b58e540b184be33d81da11aa6",
      "value": " 547/547 [00:00&lt;00:00, 13.4kB/s]"
     }
    },
    "d6fff8b41575401a8bb7f30e1efbaa35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad467b8bc1b48b6825d3329d6cc42de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d1a2215d74645a0add99885e9a9800c",
      "max": 547,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0eb27952325f4fc19dd096db42e76f27",
      "value": 547
     }
    },
    "ee035009f16e4198bc963791ddd91fe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f800a29a2cd454da5c164072c5ab374",
      "placeholder": "​",
      "style": "IPY_MODEL_0d7d3dc225ad462998e23a4137489679",
      "value": "config.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
