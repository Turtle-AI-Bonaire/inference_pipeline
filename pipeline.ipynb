{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6d87ca",
   "metadata": {},
   "source": [
    "# Google Colab Turtle Recognition Pipeline - Tag a Turtle\n",
    "## 1. Introduction\n",
    "The goal of this pipeline is to identify sea turtles by their facial features using machine learning models. It processes turtle face images, creates embeddings for these faces, and matches them to known turtle identities.\n",
    "\n",
    "## 2. Requirements\n",
    "* Environment: Google Colab\n",
    "\n",
    "* Inputs:\n",
    "A gallery of turtle images (either flat or by identity).\n",
    "A query folder with turtle images for identification.\n",
    "\n",
    "* Outputs:\n",
    "A gallery per query image showing ranked turtle images by similarity.\n",
    "\n",
    "* Modularity: \n",
    "The pipeline is designed to be modular, allowing easy swapping of models and processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29249256",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Process configuration\n",
    "Parameters that impact recognition results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3402,
   "id": "3b5f1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The resized image to center crop from for the embedding model\n",
    "resize_scale = 256\n",
    "\n",
    "# Resize scale before passing to Detection model\n",
    "full_resize_n = 1920\n",
    "\n",
    "# Combine all images of a particular turtle from the database into one for improved performance\n",
    "aggregate_dataset_identities = False\n",
    "\n",
    "# Combine all query images of the same turtle into one for improved performance\n",
    "aggregate_query_identities = False\n",
    "\n",
    "# The amount of most similar images to be shown (top 10, or top 5 most similar turtles)\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a278c",
   "metadata": {},
   "source": [
    "### Parallelism configuration\n",
    "Options that impact system speed and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3403,
   "id": "56b03d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger batch sizes speed up processing but may overflow memory.\n",
    "inference_batch_size = 16\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ec4b3",
   "metadata": {},
   "source": [
    "### Paths configuration\n",
    "Only for developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3404,
   "id": "9656d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/delta/Documents/Turtles/dataset_May15th/train/reid\"\n",
    "yolo_checkpoint_path = '/home/delta/Documents/Turtles/dataset_ops/Tag-A-Turtle/weights/best.pt'\n",
    "resnet101_checkpoint_path = \"/home/delta/Documents/Turtles/Proxy-Anchor/logs/best_wobbly-pond-226.pth\"\n",
    "query_folder_path = \"/home/delta/Documents/Turtles/Proxy-Anchor/query_images\"\n",
    "\n",
    "# Path of the embedded dataset. This embedding file has to be regenerated if the model is updated.\n",
    "saved_embeddings_path = \"./RecognitionCache/embeddings/embeddings.pth\"\n",
    "# Path of the generated metadata\n",
    "auto_metadata_path = f\"/home/delta/Documents/Turtles/Proxy-Anchor/RecognitionCache/datasets/auto_{dataset_path.split('/')[-1]}.csv\"\n",
    "metadata_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3405,
   "id": "ed2c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "error_crops = 0\n",
    "def set_error_crops(n):\n",
    "    global error_crops\n",
    "    error_crops = n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b416833",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3406,
   "id": "58828243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "from torchvision.models import resnet101\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import re\n",
    "# determine device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68a98",
   "metadata": {},
   "source": [
    "## Set the device on which to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3407,
   "id": "2d205147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to 'cuda' if a GPU is available, otherwise default to 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = device if use_gpu else \"cpu\"\n",
    "if use_gpu and device == \"cpu\":\n",
    "    print(\"GPU not available.\")\n",
    "# Verify the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7c7ed",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3408,
   "id": "9c63a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(file_path):\n",
    "    # Extract the filename from the full file path\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Check for format 1: \"03_066_R_2003_1.jpg\" (any extension, underscores included)\n",
    "    match1 = re.match(r\"(\\d{2}[-_]\\d{3})_([A-Za-z])_\\d{4}_\\d+\\.[a-zA-Z]+$\", filename)\n",
    "    if match1:\n",
    "        return [match1.group(1), match1.group(2)]\n",
    "    \n",
    "    # Check for format 2: \"03_066 R.JPG\" (any extension, underscores included)\n",
    "    match2 = re.match(r\"(\\d{2}[-_]\\d{3})\\s([A-Za-z])\\.[a-zA-Z]+$\", filename)\n",
    "    if match2:\n",
    "        return [match2.group(1), match2.group(2)]\n",
    "    \n",
    "    # Return None if no match found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3409,
   "id": "bee423ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./RecognitionCache/datasets\", exist_ok=True)\n",
    "os.makedirs(\"./RecognitionCache/embeddings\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84625b",
   "metadata": {},
   "source": [
    "## Generating a metadata for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3410,
   "id": "0e7947da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_folder_metadata(root_path):\n",
    "    file_paths = []\n",
    "    \n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            file_paths.append(os.path.join(subdir, file))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        labels = extract_labels(file_path)    \n",
    "        # Add the row to the new format data\n",
    "        data.append({\n",
    "            'bonaire_turtle_id': labels[0],\n",
    "            'side': labels[1],\n",
    "            'filename': file_path,\n",
    "        })\n",
    "    df = pd.DataFrame(data)    \n",
    "    df.to_csv(auto_metadata_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e6ae8",
   "metadata": {},
   "source": [
    "## Generating metadata for the query files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3411,
   "id": "fbfb09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_folder_metadata(root_path):\n",
    "    data = []\n",
    "    \n",
    "    # Walk through the root directory and its subdirectories\n",
    "    for subdir, _, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            # Join the subdirectory and file name to get the full path\n",
    "            data.append({\n",
    "                'bonaire_turtle_id': f\"{subdir}\",\n",
    "                'side': \"U\",\n",
    "                'filename': os.path.join(subdir, file),\n",
    "            })\n",
    "    df = pd.DataFrame(data)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf0522",
   "metadata": {},
   "source": [
    "## Make sure there is metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3412,
   "id": "faae87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_metadata():\n",
    "    data_path = None\n",
    "    \n",
    "    # Try loading user provided metadata\n",
    "    if metadata_path:\n",
    "        data_path = metadata_path\n",
    "        \n",
    "    # If it fails, load auto metadata\n",
    "    else:\n",
    "        data_path = auto_metadata_path\n",
    "    data_exists = os.path.isfile(data_path)\n",
    "    \n",
    "    # If there is no metadata, generate it\n",
    "    if not data_exists:\n",
    "        print(f\"There is no metadata file {data_path}\")\n",
    "        generate_data_folder_metadata(dataset_path)\n",
    "        data_path = auto_metadata_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629559f",
   "metadata": {},
   "source": [
    "## Resize image tensor batch from any size to size N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3413,
   "id": "0c672224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_t_to_n_padded(image_batch, n=224):\n",
    "    \"\"\"\n",
    "    Takes a batch of image tensors, resizes the smaller dimension to 'n', and pads \n",
    "    the larger dimension with black to fit a square of size n x n. Returns a batch of \n",
    "    images in tensor form.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): A tensor batch of images with shape (B, C, H, W), where B is the batch size, \n",
    "                                   C is the number of channels (3 for RGB), H is the height, and W is the width.\n",
    "        n (int): The target size for the square image (default 224).\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor batch of images, each of size n x n.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((n, n)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    batch_images = []\n",
    "    for img_tensor in image_batch:\n",
    "        img = transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "        width, height = img.size\n",
    "        if width == height:\n",
    "            # If the image is already square, simply resize\n",
    "            transformed_image = transform(img)\n",
    "        elif width < height:\n",
    "            # Resize height to n, calculate padding for width\n",
    "            new_height = n\n",
    "            new_width = int(width * (new_height / height))\n",
    "            padding_left = (n - new_width) // 2\n",
    "            padding_right = n - new_width - padding_left\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((padding_left, 0, padding_right, 0), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img) \n",
    "        else:  # height < width\n",
    "            # Resize width to n, calculate padding for height\n",
    "            new_width = n\n",
    "            new_height = int(height * (new_width / width))\n",
    "            padding_top = (n - new_height) // 2\n",
    "            padding_bottom = n - new_height - padding_top\n",
    "            transformed_image = transforms.Compose([\n",
    "                transforms.Resize((new_height, new_width)),\n",
    "                transforms.Pad((0, padding_top, 0, padding_bottom), fill=0, padding_mode='constant'),\n",
    "                transforms.ToTensor()\n",
    "            ])(img)\n",
    "\n",
    "        batch_images.append(transformed_image)\n",
    "\n",
    "    # Stack images into a single tensor batch\n",
    "    return torch.stack(batch_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3414,
   "id": "07a1253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_to_n_padded(image_pil, n=224):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((n, n)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = image_pil\n",
    "\n",
    "    width, height = img.size\n",
    "    if width == height:\n",
    "        # If the image is already square, simply resize\n",
    "        transformed_image = transform(img)\n",
    "    elif width < height:\n",
    "        # Resize height to n, calculate padding for width\n",
    "        new_height = n\n",
    "        new_width = int(width * (new_height / height))\n",
    "        padding_left = (n - new_width) // 2\n",
    "        padding_right = n - new_width - padding_left\n",
    "        transformed_image = transforms.Compose([\n",
    "            transforms.Resize((new_height, new_width)),\n",
    "            transforms.Pad((padding_left, 0, padding_right, 0), fill=0, padding_mode='constant'),\n",
    "            transforms.ToTensor()\n",
    "        ])(img) \n",
    "    else:  # height < width\n",
    "        # Resize width to n, calculate padding for height\n",
    "        new_width = n\n",
    "        new_height = int(height * (new_width / width))\n",
    "        padding_top = (n - new_height) // 2\n",
    "        padding_bottom = n - new_height - padding_top\n",
    "        transformed_image = transforms.Compose([\n",
    "            transforms.Resize((new_height, new_width)),\n",
    "            transforms.Pad((0, padding_top, 0, padding_bottom), fill=0, padding_mode='constant'),\n",
    "            transforms.ToTensor()\n",
    "        ])(img)\n",
    "\n",
    "\n",
    "    # Stack images into a single tensor batch\n",
    "    return transformed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26222d2b",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3415,
   "id": "c4ac280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TurtlesDataset(Dataset):\n",
    "    def __init__(self, mode: str, root: str = None, transform=None, ignoreThreshold=0, data=None):\n",
    "        self.mode = mode.lower()\n",
    "        self.transform = transform\n",
    "        if data is None:\n",
    "            data_path = metadata_path if metadata_path else auto_metadata_path\n",
    "            self.root = root\n",
    "            meta_path = os.path.join(self.root, data_path)\n",
    "            if not os.path.isfile(meta_path):\n",
    "                raise FileNotFoundError(f\"No metadata file found (expected {data_path}).\")\n",
    "            data_df = pd.read_csv(meta_path)\n",
    "        else:\n",
    "            data_df = data\n",
    "        cutoff = min(16, len(data_df))\n",
    "        # data_df = data_df[:cutoff]\n",
    "            \n",
    "        print(f\"Dataset size: {len(data_df)}\")\n",
    "        grouped = data_df.groupby(['bonaire_turtle_id', 'side'])\n",
    "        group_counts = grouped.size()\n",
    "\n",
    "        if ignoreThreshold > 0: \n",
    "            multi_sample_groups = group_counts[group_counts > ignoreThreshold]\n",
    "            print(\"Removing single-sample classes for BonaireTurtlesDataset.\")\n",
    "        else: multi_sample_groups = group_counts\n",
    "\n",
    "        self.im_paths, self._y_strs, self.positions = [], [], []\n",
    "        for (turtle_id, side), group_df in grouped:\n",
    "            if (turtle_id, side) not in multi_sample_groups.index:\n",
    "                continue\n",
    "\n",
    "            for _, row in group_df.iterrows():\n",
    "                filename = row.get(\"filename\", \"\").strip()\n",
    "                if not filename:\n",
    "                    continue\n",
    "\n",
    "                img_path = filename\n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "\n",
    "                identity = f\"{turtle_id}\"\n",
    "                self.im_paths.append(img_path)\n",
    "                self._y_strs.append(identity)\n",
    "                self.positions.append(side)\n",
    "\n",
    "        if not self.im_paths:\n",
    "            raise RuntimeError(\"Dataset is empty.\")\n",
    "\n",
    "        all_classes = sorted(set(self._y_strs))\n",
    "\n",
    "        filtered = [i for i, lbl in enumerate(self._y_strs) if lbl in all_classes]\n",
    "        self.im_paths = [self.im_paths[i] for i in filtered]\n",
    "        self._y_strs = [self._y_strs[i] for i in filtered]\n",
    "\n",
    "        self.classes = sorted(all_classes)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.ys = [self.class_to_idx[s] for s in self._y_strs]\n",
    "        print(\"Data length:\", len(self.ys))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.im_paths[index]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img).unsqueeze(0)\n",
    "            img = resize_images_to_n_padded(img, full_resize_n).squeeze(0)\n",
    "        target = self.ys[index]\n",
    "        return img, target\n",
    "    \n",
    "    def get_path(self, index):\n",
    "        return self.im_paths[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3416,
   "id": "0cec8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtlesPathDataset(TurtlesDataset):\n",
    "    def __init__(self, mode: str, root: str = None, transform=None, ignoreThreshold=0, data=None):\n",
    "        super().__init__(mode, root, transform, ignoreThreshold, data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.im_paths[index], self.ys[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8269d1",
   "metadata": {},
   "source": [
    "## Get Labels from Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3417,
   "id": "9832a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_labels(dataloader):\n",
    "    \"\"\"\n",
    "    This function takes a PyTorch DataLoader and returns a tensor containing all the labels.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader object containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all the labels.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Concatenate all labels into a single tensor\n",
    "    return torch.cat(all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bee72",
   "metadata": {},
   "source": [
    "## Resnet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3418,
   "id": "aed3175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Resnet101(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, is_norm=True, bn_freeze = True):\n",
    "        super(Resnet101, self).__init__()\n",
    "\n",
    "        self.model = resnet101(pretrained)\n",
    "        self.is_norm = is_norm\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.model.embedding = nn.Linear(self.num_ftrs, self.embedding_size)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-12)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "        If no object is detected, return the original image for the respective batch item.\n",
    "\n",
    "        Args:\n",
    "            image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "            The image batch should be normalized to \n",
    "            resnet_mean = [0.485, 0.456, 0.406]\n",
    "            resnet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "        \"\"\"\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model.embedding(x)\n",
    "        \n",
    "        if self.is_norm:\n",
    "            x = self.l2_norm(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding.weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3fd8c",
   "metadata": {},
   "source": [
    "## Resnet101 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3419,
   "id": "e7ca20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/delta/penv/aienv/lib/python3.12/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/delta/penv/aienv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resnet101(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (gmp): AdaptiveMaxPool2d(output_size=1)\n",
       "    (embedding): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet101_model = Resnet101(embedding_size=512, pretrained=True, is_norm=True, bn_freeze = True)\n",
    "\n",
    "# Path to checkpoint\n",
    "checkpoint = torch.load(resnet101_checkpoint_path)\n",
    "resnet101_model.load_state_dict(checkpoint)\n",
    "resnet101_model.to(device)\n",
    "resnet101_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d87e7",
   "metadata": {},
   "source": [
    "## YOLO Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3420,
   "id": "c8d25a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/delta/Documents/Turtles/dataset_ops/Tag-A-Turtle/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "print(yolo_checkpoint_path)\n",
    "yolo_model = YOLO(yolo_checkpoint_path).cuda()  # You can specify a different model if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90961b44",
   "metadata": {},
   "source": [
    "## Batch Process image tensors with YOLO\n",
    "The function below takes a image tensor batch. The images are then cropped around the turtle head. \n",
    "If no turtle head is found the original image is used.\n",
    "\n",
    "The function returns the tensor batch, cropped around the turtles' heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3421,
   "id": "db6bbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def batch_crop_yolov11(image_batch: torch.Tensor, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    global error_crops\n",
    "    \"\"\"\n",
    "    Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "    If no object is detected, return the original image for the respective batch item.\n",
    "    Additionally, display the image with bounding boxes if there are more than 2 boxes.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        model (YOLO): Preloaded YOLOv11 model.\n",
    "        conf_threshold (float): Confidence threshold for detections.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the image batch\n",
    "    images = image_batch.to(device)\n",
    "    print(\"images shape\",images.shape)\n",
    "    # Perform inference\n",
    "    results = model.predict(source=images, conf=conf_threshold, device=device, verbose=False)\n",
    "\n",
    "    # Initialize a list to store cropped images\n",
    "    cropped_images = []\n",
    "\n",
    "    # Iterate over each result\n",
    "    for result, image in zip(results, images):\n",
    "        if result.boxes is not None and len(result.boxes.xyxy) > 0:\n",
    "            boxes = result.boxes.xyxy.int().cpu().tolist()\n",
    "\n",
    "            # if len(boxes) > 2:\n",
    "            #     # Convert the image to PIL format for display\n",
    "            #     pil_image = transforms.ToPILImage()(image.cpu())\n",
    "            #     draw = ImageDraw.Draw(pil_image)\n",
    "            #     # Draw each box on the image\n",
    "            #     for box in boxes:\n",
    "            #         x_min, y_min, x_max, y_max = box\n",
    "            #         draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=3)\n",
    "\n",
    "            #     # Show the image with bounding boxes\n",
    "            #     plt.imshow(pil_image)\n",
    "            #     plt.show()\n",
    "            if len(boxes) != 1:\n",
    "                error_crops = error_crops + 1\n",
    "                \n",
    "            # Crop the boxes and add them to the list\n",
    "            x_min, y_min, x_max, y_max = boxes[0]   \n",
    "            cropped_image = image[:, y_min:y_max, x_min:x_max]\n",
    "            img = resize_images_to_n_padded(cropped_image.unsqueeze(0), resize_scale)\n",
    "            cropped_images.append(img.squeeze(0))\n",
    "                \n",
    "        else:\n",
    "            # If no detections, append the original image to the batch\n",
    "            img = resize_images_to_n_padded(image.unsqueeze(0), resize_scale)\n",
    "            cropped_images.append(img.squeeze(0))\n",
    "\n",
    "    # Stack the cropped images into a single tensor\n",
    "    cropped_images_tensor = torch.stack(cropped_images)\n",
    "\n",
    "    return cropped_images_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93814c",
   "metadata": {},
   "source": [
    "## Single Yolo Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3422,
   "id": "0d962cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_crop_yolov11(image_name: str, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    global error_crops\n",
    "    \"\"\"\n",
    "    Crop and return a batch of cropped images from the input tensor batch using YOLOv11.\n",
    "    If no object is detected, return the original image for the respective batch item.\n",
    "    Additionally, display the image with bounding boxes if there are more than 2 boxes.\n",
    "\n",
    "    Args:\n",
    "        image_batch (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        model (YOLO): Preloaded YOLOv11 model.\n",
    "        conf_threshold (float): Confidence threshold for detections.\n",
    "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of cropped images or original images if no detections.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image batch\n",
    "    # Perform inference\n",
    "    results = model.predict(source=image_name, conf=conf_threshold, device=device, verbose=False)\n",
    "    img = Image.open(image_name).convert(\"RGB\")\n",
    "\n",
    "    # Initialize a list to store cropped images\n",
    "    for result in results:\n",
    "        if result.boxes is not None and len(result.boxes.xyxy) > 0:\n",
    "            # Get the total number of boxes\n",
    "            num_boxes = len(result.boxes.xyxy)\n",
    "            if num_boxes != 1:\n",
    "                error_crops = error_crops + 1\n",
    "            # Get the first detected box (if there is at least one detection)\n",
    "            first_box = result.boxes.xyxy[0].int().cpu().tolist()  # Convert to a list of integers (x_min, y_min, x_max, y_max)\n",
    "            x_min, y_min, x_max, y_max = first_box\n",
    "\n",
    "            img = img.crop((x_min, y_min, x_max, y_max))\n",
    "        else: error_crops += 1\n",
    "            \n",
    "        # If no detections, append the original image to the batch            \n",
    "    img = resize_image_to_n_padded(img, resize_scale)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3423,
   "id": "d1d7d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_crop_name(image_names: torch.Tensor, model: YOLO, conf_threshold: float = 0.5, device: str = 'cuda'):\n",
    "    cropped_images = []\n",
    "    for image_name in image_names:\n",
    "        cropped_image = single_crop_yolov11(image_name, model, conf_threshold, device)\n",
    "        cropped_images.append(cropped_image)\n",
    "    return torch.stack(cropped_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340af22",
   "metadata": {},
   "source": [
    "## Standard Transform before Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3424,
   "id": "a031037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(original = 256, output=224):\n",
    "    resnet_sz_resize = original\n",
    "    resnet_sz_crop = output\n",
    "    resnet_mean = [0.485, 0.456, 0.406]\n",
    "    resnet_std = [0.229, 0.224, 0.225]\n",
    "    resnet_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.Resize(resnet_sz_resize),\n",
    "        transforms.CenterCrop(resnet_sz_crop),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=resnet_mean, std=resnet_std)\n",
    "    ])\n",
    "    return resnet_transform\n",
    "resnet_transform = make_transform(original=resize_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c211bc",
   "metadata": {},
   "source": [
    "## Resize, crop and normalize tensors for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3425,
   "id": "66b44369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_embed_transform(image_tensor_batch):\n",
    "    processed_images = []\n",
    "    for image_tensor in image_tensor_batch:\n",
    "        processed_images.append(resnet_transform(image_tensor))\n",
    "    return torch.stack(processed_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe978b1",
   "metadata": {},
   "source": [
    "## Inference Pipeline\n",
    "Handles tensorized images in batches, applying all the inferences and transforms required to produce the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3426,
   "id": "540ce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_pipeline(image_tensor):\n",
    "    global yolo_model, resnet101_model, error_crops\n",
    "    # Detect face in the image\n",
    "    # faces_tensor = batch_crop_yolov11(image_tensor, yolo_model)\n",
    "    faces_tensor = batch_crop_name(image_tensor, yolo_model, conf_threshold=0.5)\n",
    "    # print(\"Number of cropping errors\", error_crops)\n",
    "    \n",
    "    # crop and normalize before embedding model\n",
    "    normalized_face_tensor = pre_embed_transform(faces_tensor).to(device)\n",
    "    del faces_tensor\n",
    "    \n",
    "    # Generate embedding for the detected face\n",
    "    embeddings = resnet101_model(normalized_face_tensor)\n",
    "    del normalized_face_tensor  \n",
    "    \n",
    "    return embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3427,
   "id": "36984039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrorRate():\n",
    "    global error_crops\n",
    "    return error_crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf8885",
   "metadata": {},
   "source": [
    "## Function to run the inference pipeline over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3428,
   "id": "66bd90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_to_embeddings(data_loader):\n",
    "    global inference_batch_size\n",
    "    processed_batches = []\n",
    "    pbar = tqdm(data_loader, desc=\"Processing data\")\n",
    "    index = 1\n",
    "    for values in pbar:\n",
    "        error_rate_percentage = (getErrorRate() / (index * inference_batch_size)) * 100\n",
    "        formatted_error_rate = f\"{error_rate_percentage:.3f}%\"\n",
    "        pbar.set_postfix({\"Err Rate\": formatted_error_rate})\n",
    "        processed_batch = infer_pipeline(values[0])\n",
    "        processed_batches.append(processed_batch.cpu())\n",
    "        index += 1\n",
    "    \n",
    "    return torch.cat(processed_batches, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3429,
   "id": "366d5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def turtle_similarities(query_embedding, gallery_embeddings):\n",
    "    cosine_sim = F.cosine_similarity(query_embedding, gallery_embeddings)\n",
    "    \n",
    "    return cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3430,
   "id": "af415e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_recognition_results(query_image, similar_images, titles):\n",
    "    # Increase the figure size to make images 3x larger than the previous size\n",
    "    plt.figure(figsize=(90, 45))  # 3x larger than the previous figsize\n",
    "    plt.subplot(1, len(similar_images)+1, 1)\n",
    "    print(query_image[0])\n",
    "    \n",
    "    if type(query_image[0]) == str:\n",
    "        query_image = mpimg.imread(query_image[0])\n",
    "    plt.imshow(query_image)\n",
    "    plt.title(\"Query\")\n",
    "    \n",
    "    for idx, sim_img in enumerate(similar_images):\n",
    "        plt.subplot(1, len(similar_images)+1, idx+2)\n",
    "        if type(sim_img[0]) == str:\n",
    "            sim_img = mpimg.imread(sim_img)\n",
    "        plt.imshow(sim_img)\n",
    "        plt.title(titles[idx])\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e1ab1",
   "metadata": {},
   "source": [
    "## Embeddings aggregator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3431,
   "id": "1cd8e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Average embeddings in the batch per turtle identity to produce more accurate embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : str\n",
    "        'mean' (default) or 'weighted'.\n",
    "        When 'weighted', `weights` must be supplied in forward().\n",
    "    return_index_map : bool\n",
    "        If True, also returns a tensor that maps each original sample\n",
    "        to its aggregated row – handy for gathering losses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = \"mean\", return_index_map: bool = False):\n",
    "        super().__init__()\n",
    "        assert reduction in {\"mean\", \"weighted\"}\n",
    "        self.reduction = reduction\n",
    "        self.return_index_map = return_index_map\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        emb: torch.Tensor,             # shape (B, D)\n",
    "        labels: torch.Tensor,          # shape (B,)\n",
    "        weights: torch.Tensor | None = None\n",
    "    ):\n",
    "        device = emb.device\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # ---- gather bookkeeping -------------------------------------------------\n",
    "        # unique ids and position of each sample’s aggregated row\n",
    "        uniq_ids, inv = torch.unique(labels, return_inverse=True)\n",
    "        num_ids = uniq_ids.size(0)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            # quick path – use scatter_add then divide by counts\n",
    "            summed = torch.zeros(num_ids, emb.size(1), device=device).scatter_add_(0,\n",
    "                      inv.unsqueeze(-1).expand_as(emb), emb)\n",
    "            counts = torch.bincount(inv, minlength=num_ids).unsqueeze(1)\n",
    "            agg = summed / counts.clamp_min(1)\n",
    "\n",
    "        else:  # weighted mean\n",
    "            assert weights is not None, \"`weights` required for weighted reduction\"\n",
    "            weights = weights.to(device).unsqueeze(1)  # (B, 1)\n",
    "            w_sum = torch.zeros(num_ids, 1, device=device).scatter_add_(0, inv.unsqueeze(-1), weights)\n",
    "            w_emb = torch.zeros_like(summed).scatter_add_(0, inv.unsqueeze(-1).expand_as(emb), emb * weights)\n",
    "            agg = w_emb / w_sum.clamp_min(1e-8)\n",
    "\n",
    "        if self.return_index_map:\n",
    "            return agg, uniq_ids, inv\n",
    "        return agg, uniq_ids, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6665ca3",
   "metadata": {},
   "source": [
    "## Sort embeddings by similarity with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3432,
   "id": "cbcb8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_em_by_sim(similarities, query_labels):\n",
    "    similarities_and_labels = [(similarity.item(), query_labels[i]) for i, similarity in enumerate(similarities)]\n",
    "\n",
    "    # Sort the list by similarity in descending order\n",
    "    similarities_and_labels_sorted = sorted(similarities_and_labels, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Optionally, separate the sorted similarities and labels back into two lists/tensors\n",
    "    sorted_similarities = [pair[0] for pair in similarities_and_labels_sorted]\n",
    "    sorted_labels = [pair[1] for pair in similarities_and_labels_sorted]\n",
    "\n",
    "    return sorted_similarities, sorted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6a06c",
   "metadata": {},
   "source": [
    "## Retrieve the images in a dataset that match the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3433,
   "id": "5760ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_indices(string_list, query):\n",
    "    # Create a list of indices where the string matches the query\n",
    "    matching_indices = [i for i, s in enumerate(string_list) if s == query]\n",
    "    return matching_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be01ec",
   "metadata": {},
   "source": [
    "## Basic tensor transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3434,
   "id": "a9cc593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109405ba",
   "metadata": {},
   "source": [
    "## Main Pipeline Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8396\n",
      "Data length: 8396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  37%|███▋      | 193/525 [03:03<09:35,  1.73s/it, Err Rate=1.289%]"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings_exist = os.path.exists(saved_embeddings_path)\n",
    "    error_crops = 0\n",
    "    # Load dataset embeddings\n",
    "    dataset_embeddings = None\n",
    "    if embeddings_exist:\n",
    "        dataset_embeddings, dataset_labels = torch.load(saved_embeddings_path)\n",
    "    else:\n",
    "        generate_data_folder_metadata(dataset_path)\n",
    "        dataset = TurtlesPathDataset(root=dataset_path, mode=\"all\", transform=to_tensor_transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "        dataset_embeddings = process_images_to_embeddings(data_loader)\n",
    "        dataset_labels = get_dataloader_labels(data_loader)\n",
    "        torch.save((dataset_embeddings, dataset_labels), saved_embeddings_path)\n",
    "\n",
    "    # Load query embeddings\n",
    "    query_metadata = generate_query_folder_metadata(query_folder_path)\n",
    "    print(f\"Found {len(query_metadata)} query images\")\n",
    "    query_dataset = TurtlesPathDataset(data=query_metadata, mode=\"all\", transform=to_tensor_transform)\n",
    "    query_loader = DataLoader(query_dataset, batch_size=inference_batch_size, pin_memory=True, shuffle=False, num_workers=1)\n",
    "    query_embeddings = process_images_to_embeddings(query_loader)\n",
    "    query_labels = get_dataloader_labels(query_loader)\n",
    "\n",
    "    # Aggregate embeddings by identity if enabled\n",
    "    aggregator = EmbeddingAggregator().to(device)\n",
    "    if aggregate_dataset_identities:\n",
    "        dataset_embeddings, dataset_labels, _ = aggregator(dataset_embeddings, labels=dataset_labels)\n",
    "    if aggregate_query_identities:\n",
    "        query_embeddings, query_labels, _ = aggregator(query_embeddings, labels=query_labels)\n",
    "        \n",
    "    for query_index in range(query_embeddings.shape[0]):\n",
    "        query_em = query_embeddings[query_index]\n",
    "        query_label = query_labels[query_index]\n",
    "        \n",
    "        # Get the similarities between query and dataset embeddings\n",
    "        similarities = turtle_similarities(query_em, dataset_embeddings)\n",
    "        \n",
    "        # Extract image paths from dataset\n",
    "        image_locations = [dataset[i][0] for i in range(len(dataset))]  # image paths\n",
    "\n",
    "        # Sort the similarities and get the top N results\n",
    "        top_n = min(top_n, len(similarities))\n",
    "        sorted_similarities, sorted_labels = sort_em_by_sim(similarities, image_locations)[:top_n]\n",
    "        \n",
    "        # Prepare images and filenames to be plotted\n",
    "        images = []\n",
    "        images_labels = []\n",
    "        for label in sorted_labels:\n",
    "            images.append(label)  # image path\n",
    "            images_labels.append(os.path.basename(label))  # Get the filename from the image path\n",
    "        # Find the query image path and plot results\n",
    "        query_image_index = find_matching_indices(query_labels, query_label)[0]\n",
    "        query_image = query_dataset[query_image_index]\n",
    "        plot_recognition_results(query_image, images, images_labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de503b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
